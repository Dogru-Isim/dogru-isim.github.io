---
layout: post
title: "LLMs and Vulnerability Research"
date: 2025-12-23
categories: articles
---

## Content

My thoughts about the place of LLMs in vulnerability research.

My thoughts about questions that came up that bug me (MindBugs).

The main questions are:

- can LLMs improve productivity in vulnerability research
  
  - how should we handle false positives
  
  - how are LLMs useful in vulnerability research
  
  - how are LLMs NOT useful in vulnerability research

- how can we (hackers) use LLMs "the right way"

## Backstory

The HackTheBox University CTF 2025 ended yesterday. I focused on the web category and honestly, I was expecting it to be easier. According to one of my teammates, the OSINT challenges were very easy. I can't testify to that though.

Regardless, the 3 web challenges (ranked "Very Easy, Easy, and Medium") were quite humbling. I was only able to solve one of them. Although by this time, some teams had already solved all the challenges. Impressive. Moreover, according to another teammate, some teams had submitted 30/34 flags within 30 minutes into the CTF. This was so impressive to us that one of my teammates asked "Are they using automated hacking tools?".

Of course, it doesn't matter what the winner team does since we were not competing for the first rank, or any rank whatsoever. I personally refrained from using such tools to "prove my own skills". It was only after my team got the flag or after the CTF ended did I actually throw the source code to an LLM.

While the CTF was live and no writeups were published, I solved the first web challenge and then threw the source code into ChatGPT and asked "Find the vulnerability that allows a user to register a new user to wordpress here". ChatGPT was able to find the authentication bypass vulnerability after some back and forth. But it couldn't chain the authentication bypass to the second vulnerability (arbitrary Wordpress option update). In fact, it falsely stated that it was not possible to register a new user. Interestingly, I asked the same question with the same source code after the CTF ended and writups got published (and I showed ChatGPT the solution in a different chat). As expected, ChatGPT found the exploitation chain immediately.

After the event ended, I asked similar questions to Claude. Like ChatGPT it failed to show the real impact of the vulnerabilities that it identified. This made me wonder "How is it that the LLMs can find the vulnerable lines seemingly so easily but hallucinate when it comes to showing the impact?" (MindBug-001) And sometimes, it showed only the most critical vulnerability that it "thought" it found while skipping the real vulnerability. It identified the real vulnerability after I said "Find all the vulnerabilities" instead of "Find the vulnerability". This raised another question "Is it the way that I ask the questions that result in bad output?" (MindBug-002)

Additionally, after my team submitted the flag for the second challenge, I asked Claude to find the vulnerability in the challenge's source code with zero help from my side. And it failed miserably. It did find the vulnerability after online writeups were published though. Which strengthened my suspicions about unreliable metrics when it comes to source code analysis. "How good are LLMs at solving problems that don't already have a solution on the web?" (MindBug-003)
